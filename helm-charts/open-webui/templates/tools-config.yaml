apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-tools-config
  namespace: {{ .Values.release.namespace }}
data:
  tools.json: |
    {
      "tools": [
        {
          "name": "search_documentation",
          "description": "Поиск в технической документации",
          "parameters": {
            "query": "string",
            "max_results": "integer"
          },
          "code": "async def search_documentation(query: str, max_results: int = 5) -> list:\n    # Реализация поиска\n    pass"
        },
        {
          "name": "code_analyzer",
          "description": "Анализ и оптимизация кода",
          "parameters": {
            "code": "string",
            "language": "string"
          },
          "code": "async def code_analyzer(code: str, language: str) -> dict:\n    # Анализ кода\n    pass"
        },
        {
          "name": "memory_optimizer",
          "description": "Оптимизация использования памяти",
          "parameters": {
            "process_id": "integer"
          },
          "code": "async def memory_optimizer(process_id: int) -> dict:\n    # Оптимизация памяти\n    pass"
        },
        {
          "name": "context_manager",
          "description": "Управление контекстом диалога",
          "parameters": {
            "context_size": "integer",
            "priority": "string"
          },
          "code": "async def context_manager(context_size: int, priority: str) -> dict:\n    # Управление контекстом\n    pass"
        },
        {
          "name": "token_analyzer",
          "description": "Анализ использования токенов",
          "parameters": {
            "text": "string",
            "model": "string"
          },
          "code": "async def token_analyzer(text: str, model: str) -> dict:\n    # Анализ токенов\n    pass"
        },
        {
          "name": "prompt_optimizer",
          "description": "Оптимизация промптов",
          "parameters": {
            "prompt": "string",
            "target_tokens": "integer"
          },
          "code": "async def prompt_optimizer(prompt: str, target_tokens: int) -> str:\n    # Оптимизация промптов\n    pass"
        },
        {
          "name": "conversation_summarizer",
          "description": "Создание саммари диалога",
          "parameters": {
            "conversation": "string",
            "max_length": "integer"
          },
          "code": "async def conversation_summarizer(conversation: str, max_length: int) -> str:\n    # Создание саммари\n    pass"
        },
        {
          "name": "semantic_search",
          "description": "Семантический поиск по истории",
          "parameters": {
            "query": "string",
            "threshold": "float"
          },
          "code": "async def semantic_search(query: str, threshold: float) -> list:\n    # Семантический поиск\n    pass"
        },
        {
          "name": "style_adapter",
          "description": "Адаптация стиля общения",
          "parameters": {
            "text": "string",
            "style": "string"
          },
          "code": "async def style_adapter(text: str, style: str) -> str:\n    # Адаптация стиля\n    pass"
        },
        {
          "name": "format_converter",
          "description": "Конвертация форматов ответов",
          "parameters": {
            "content": "string",
            "target_format": "string"
          },
          "code": "async def format_converter(content: str, target_format: str) -> str:\n    # Конвертация форматов\n    pass"
        },
        {
          "name": "template_manager",
          "description": "Управление шаблонами ответов",
          "parameters": {
            "template_id": "string",
            "variables": "object"
          },
          "code": "async def template_manager(template_id: str, variables: dict) -> str:\n    # Управление шаблонами\n    pass"
        },
        {
          "name": "error_analyzer",
          "description": "Анализ ошибок в ответах",
          "parameters": {
            "response": "string",
            "context": "string"
          },
          "code": "async def error_analyzer(response: str, context: str) -> dict:\n    # Анализ ошибок\n    pass"
        },
        {
          "name": "knowledge_validator",
          "description": "Валидация знаний",
          "parameters": {
            "statement": "string",
            "sources": "array"
          },
          "code": "async def knowledge_validator(statement: str, sources: list) -> dict:\n    # Валидация знаний\n    pass"
        },
        {
          "name": "model_performance_analyzer",
          "description": "Анализ производительности и использования ресурсов модели",
          "parameters": {
            "model_name": "string",
            "time_period": "integer",
            "metrics": "array"
          },
          "code": "async def model_performance_analyzer(model_name: str, time_period: int, metrics: list) -> dict:\n    # Анализ производительности модели\n    pass"
        },
        {
          "name": "context_window_optimizer",
          "description": "Оптимизация размера контекстного окна для различных моделей",
          "parameters": {
            "model_name": "string",
            "text_length": "integer",
            "memory_limit": "integer"
          },
          "code": "async def context_window_optimizer(model_name: str, text_length: int, memory_limit: int) -> dict:\n    # Оптимизация контекстного окна\n    pass"
        },
        {
          "name": "model_switcher",
          "description": "Интеллектуальное переключение между моделями на основе запроса",
          "parameters": {
            "query": "string",
            "available_models": "array",
            "requirements": "object"
          },
          "code": "async def model_switcher(query: str, available_models: list, requirements: dict) -> str:\n    # Выбор оптимальной модели\n    pass"
        },
        {
          "name": "response_formatter",
          "description": "Форматирование и структурирование ответов модели",
          "parameters": {
            "response": "string",
            "format": "string",
            "style": "string"
          },
          "code": "async def response_formatter(response: str, format: str, style: str) -> str:\n    # Форматирование ответа\n    pass"
        },
        {
          "name": "conversation_archiver",
          "description": "Архивация и индексация истории диалогов",
          "parameters": {
            "conversation_id": "string",
            "metadata": "object",
            "tags": "array"
          },
          "code": "async def conversation_archiver(conversation_id: str, metadata: dict, tags: list) -> dict:\n    # Архивация диалога\n    pass"
        },
        {
          "name": "prompt_template_manager",
          "description": "Управление и применение шаблонов промптов",
          "parameters": {
            "template_name": "string",
            "variables": "object",
            "model": "string"
          },
          "code": "async def prompt_template_manager(template_name: str, variables: dict, model: str) -> str:\n    # Управление шаблонами\n    pass"
        },
        {
          "name": "token_usage_tracker",
          "description": "Отслеживание и оптимизация использования токенов",
          "parameters": {
            "text": "string",
            "model": "string",
            "max_tokens": "integer"
          },
          "code": "async def token_usage_tracker(text: str, model: str, max_tokens: int) -> dict:\n    # Отслеживание токенов\n    pass"
        },
        {
          "name": "model_config_manager",
          "description": "Управление конфигурациями различных моделей",
          "parameters": {
            "model_name": "string",
            "config": "object",
            "validate": "boolean"
          },
          "code": "async def model_config_manager(model_name: str, config: dict, validate: bool) -> dict:\n    # Управление конфигурациями\n    pass"
        },
        {
          "name": "response_validator",
          "description": "Валидация и проверка качества ответов модели",
          "parameters": {
            "response": "string",
            "criteria": "object",
            "threshold": "float"
          },
          "code": "async def response_validator(response: str, criteria: dict, threshold: float) -> dict:\n    # Валидация ответов\n    pass"
        },
        {
          "name": "deepseek_optimizer",
          "description": "Оптимизация параметров для модели deepseek-r1:14b",
          "parameters": {
          "model_config": "object",
          "performance_metrics": "object"
          },
          "code": "async def deepseek_optimizer(model_config: dict, performance_metrics: dict) -> dict:\n    try:\n        optimized_config = optimize_model_parameters(model_config, performance_metrics)\n        return {\n            'optimized_config': optimized_config,\n            'estimated_performance': calculate_performance_impact(optimized_config)\n        }\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "memory_profiler",
          "description": "Профилирование использования памяти",
          "parameters": {
          "process_name": "string",
          "duration": "integer"
          },
          "code": "async def memory_profiler(process_name: str, duration: int) -> dict:\n    try:\n        profile_data = collect_memory_metrics(process_name, duration)\n        return analyze_memory_usage(profile_data)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "gpu_optimizer",
          "description": "Оптимизация использования GPU",
          "parameters": {
          "gpu_config": "object",
          "workload_type": "string"
          },
          "code": "async def gpu_optimizer(gpu_config: dict, workload_type: str) -> dict:\n    try:\n        return optimize_gpu_settings(gpu_config, workload_type)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "batch_size_optimizer",
          "description": "Оптимизация размера батча",
          "parameters": {
          "current_batch_size": "integer",
          "memory_limit": "integer"
          },
          "code": "async def batch_size_optimizer(current_batch_size: int, memory_limit: int) -> dict:\n    try:\n        return calculate_optimal_batch_size(current_batch_size, memory_limit)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "context_window_manager",
          "description": "Управление размером контекстного окна",
          "parameters": {
          "text_length": "integer",
          "max_context": "integer"
          },
          "code": "async def context_window_manager(text_length: int, max_context: int) -> dict:\n    try:\n        return optimize_context_window(text_length, max_context)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "performance_monitor",
          "description": "Мониторинг производительности модели",
          "parameters": {
          "metrics": "array",
          "interval": "integer"
          },
          "code": "async def performance_monitor(metrics: list, interval: int) -> dict:\n    try:\n        return collect_performance_metrics(metrics, interval)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "resource_allocator",
          "description": "Оптимальное распределение ресурсов",
          "parameters": {
          "available_resources": "object",
          "priority": "string"
          },
          "code": "async def resource_allocator(available_resources: dict, priority: str) -> dict:\n    try:\n        return allocate_resources(available_resources, priority)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "cache_optimizer",
          "description": "Оптимизация кэширования",
          "parameters": {
          "cache_config": "object",
          "usage_patterns": "array"
          },
          "code": "async def cache_optimizer(cache_config: dict, usage_patterns: list) -> dict:\n    try:\n        return optimize_cache_settings(cache_config, usage_patterns)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "thread_manager",
          "description": "Управление потоками обработки",
          "parameters": {
          "thread_count": "integer",
          "workload": "object"
          },
          "code": "async def thread_manager(thread_count: int, workload: dict) -> dict:\n    try:\n        return optimize_thread_allocation(thread_count, workload)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "response_time_optimizer",
          "description": "Оптимизация времени отклика",
          "parameters": {
          "current_latency": "integer",
          "target_latency": "integer"
          },
          "code": "async def response_time_optimizer(current_latency: int, target_latency: int) -> dict:\n    try:\n        return optimize_response_time(current_latency, target_latency)\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "deepseek_r1_optimizer",
          "description": "Оптимизация параметров для модели deepseek-r1:14b-qwen-distill-q4_K_M",
          "parameters": {
            "context_length": "integer",
            "temperature": "float",
            "top_p": "float"
          },
          "code": "async def deepseek_r1_optimizer(context_length: int, temperature: float, top_p: float) -> dict:\n    try:\n        # Оптимальные параметры для deepseek-r1:14b-qwen-distill-q4_K_M\n        optimal_params = {\n            'temperature': max(0.1, min(1.0, temperature)),\n            'top_p': max(0.1, min(1.0, top_p)),\n            'context_length': min(32768, context_length),\n            'repeat_penalty': 1.1,\n            'frequency_penalty': 0.0,\n            'presence_penalty': 0.0,\n            'mirostat': 0,\n            'mirostat_tau': 5.0,\n            'mirostat_eta': 0.1\n        }\n        return optimal_params\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "long_context_handler",
          "description": "Обработка длинных контекстов для стабильной работы с большими диалогами",
          "parameters": {
            "conversation_history": "string",
            "max_tokens": "integer",
            "priority": "string"
          },
          "code": "async def long_context_handler(conversation_history: str, max_tokens: int, priority: str) -> dict:\n    try:\n        # Анализ и оптимизация длинного контекста\n        tokens_count = estimate_tokens(conversation_history)\n        if tokens_count > max_tokens:\n            if priority == 'recent':\n                # Сохраняем последние сообщения\n                optimized_history = truncate_history_keep_recent(conversation_history, max_tokens)\n            elif priority == 'important':\n                # Сохраняем важные части диалога\n                optimized_history = extract_important_parts(conversation_history, max_tokens)\n            else:\n                # Сжимаем историю с сохранением смысла\n                optimized_history = compress_history(conversation_history, max_tokens)\n            \n            return {\n                'original_tokens': tokens_count,\n                'optimized_tokens': estimate_tokens(optimized_history),\n                'optimized_history': optimized_history\n            }\n        return {'status': 'ok', 'tokens': tokens_count}\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "memory_usage_optimizer",
          "description": "Оптимизация использования памяти для стабильной работы с большими диалогами",
          "parameters": {
            "current_memory_usage": "integer",
            "max_memory": "integer"
          },
          "code": "async def memory_usage_optimizer(current_memory_usage: int, max_memory: int) -> dict:\n    try:\n        # Расчет оптимальных параметров использования памяти\n        memory_percentage = (current_memory_usage / max_memory) * 100\n        \n        if memory_percentage > 90:\n            # Критическое использование памяти\n            return {\n                'status': 'critical',\n                'recommendations': [\n                    'Уменьшить размер контекстного окна',\n                    'Активировать агрессивный сборщик мусора',\n                    'Уменьшить количество параллельных запросов до 1',\n                    'Очистить кэш модели'\n                ],\n                'settings': {\n                    'ENABLE_AGGRESSIVE_GC': 'true',\n                    'MAX_PARALLEL_REQUESTS': '1',\n                    'CHUNK_SIZE': '256',\n                    'STREAM_CHUNK_SIZE': '256'\n                }\n            }\n        elif memory_percentage > 75:\n            # Высокое использование памяти\n            return {\n                'status': 'high',\n                'recommendations': [\n                    'Оптимизировать размер контекстного окна',\n                    'Уменьшить количество параллельных запросов',\n                    'Включить периодическую очистку памяти'\n                ],\n                'settings': {\n                    'ENABLE_MEMORY_REDUCTION': 'true',\n                    'MAX_PARALLEL_REQUESTS': '1',\n                    'CHUNK_SIZE': '512',\n                    'STREAM_CHUNK_SIZE': '512'\n                }\n            }\n        else:\n            # Нормальное использование памяти\n            return {\n                'status': 'normal',\n                'recommendations': [\n                    'Текущие настройки оптимальны'\n                ],\n                'settings': {\n                    'ENABLE_MEMORY_REDUCTION': 'true',\n                    'MAX_PARALLEL_REQUESTS': '1',\n                    'CHUNK_SIZE': '512',\n                    'STREAM_CHUNK_SIZE': '512'\n                }\n            }\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "streaming_optimizer",
          "description": "Оптимизация потоковой передачи ответов для улучшения отзывчивости интерфейса",
          "parameters": {
            "response_size": "integer",
            "client_type": "string"
          },
          "code": "async def streaming_optimizer(response_size: int, client_type: str) -> dict:\n    try:\n        # Оптимизация параметров потоковой передачи\n        base_chunk_size = 512\n        \n        if response_size > 10000:\n            # Очень длинные ответы\n            if client_type == 'mobile':\n                chunk_size = 256\n            else:\n                chunk_size = 512\n        elif response_size > 5000:\n            # Длинные ответы\n            if client_type == 'mobile':\n                chunk_size = 384\n            else:\n                chunk_size = 768\n        else:\n            # Обычные ответы\n            if client_type == 'mobile':\n                chunk_size = 512\n            else:\n                chunk_size = 1024\n        \n        return {\n            'chunk_size': chunk_size,\n            'estimated_chunks': response_size // chunk_size,\n            'settings': {\n                'CHUNK_SIZE': str(chunk_size),\n                'STREAM_CHUNK_SIZE': str(chunk_size),\n                'KEEP_ALIVE': '1200',\n                'GRACEFUL_TIMEOUT': '1200'\n            }\n        }\n    except Exception as e:\n        return {'error': str(e)}"
        },
        {
          "name": "ui_responsiveness_enhancer",
          "description": "Улучшение отзывчивости пользовательского интерфейса при длинных ответах",
          "parameters": {
            "ui_config": "object",
            "expected_response_size": "integer"
          },
          "code": "async def ui_responsiveness_enhancer(ui_config: dict, expected_response_size: int) -> dict:\n    try:\n        # Оптимизация настроек UI для длинных ответов\n        if expected_response_size > 15000:\n            # Очень длинные ответы\n            return {\n                'progressive_rendering': True,\n                'chunk_rendering': True,\n                'defer_images': True,\n                'lazy_loading': True,\n                'background_processing': True,\n                'throttle_updates': 100,  # ms\n                'use_virtual_scroll': True,\n                'max_visible_messages': 15\n            }\n        elif expected_response_size > 7500:\n            # Длинные ответы\n            return {\n                'progressive_rendering': True,\n                'chunk_rendering': True,\n                'defer_images': False,\n                'lazy_loading': True,\n                'background_processing': True,\n                'throttle_updates': 50,  # ms\n                'use_virtual_scroll': True,\n                'max_visible_messages': 20\n            }\n        else:\n            # Обычные ответы\n            return {\n                'progressive_rendering': True,\n                'chunk_rendering': False,\n                'defer_images': False,\n                'lazy_loading': False,\n                'background_processing': False,\n                'throttle_updates': 0,  # no throttling\n                'use_virtual_scroll': False,\n                'max_visible_messages': 50\n            }\n    except Exception as e:\n        return {'error': str(e)}"
        }
        ]
      }