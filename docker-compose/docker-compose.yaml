services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - /mnt/o:/root/.ollama  # Изменено на путь к смонтированному диску
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_COMPUTE_TYPE=gpu
      - OLLAMA_GPU_LAYERS=99
      - OLLAMA_F16=true
      - OLLAMA_QUANTIZATION=q4_K_M
      - OLLAMA_CUDA_MEMORY_FRACTION=0.95
      - OLLAMA_CUDA_FORCE_ALLOCATION=true
      - OLLAMA_MODEL=deepseek-r1:14b-qwen-distill-q4_K_M
      - OLLAMA_CONTEXT_SIZE=8192
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - ollama-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # open-webui:
  #   image: ghcr.io/open-webui/open-webui:latest
  #   container_name: open-webui
  #   restart: unless-stopped
  #   ports:
  #     - "3000:3000"  # Проброс порта на хост-машину для доступа из локальной сети
  #   environment:
  #     - OLLAMA_API_BASE=http://ollama:11434/api
  #     - OLLAMA_BASE_URL=http://ollama:11434
  #     - OLLAMA_HOST=ollama
  #     - OLLAMA_PORT=11434
  #     - HOST=0.0.0.0  # Важно для доступа извне контейнера
  #     - MODEL=deepseek-r1:14b-qwen-distill-q4_K_M
  #     - DEBUG=false
  #     - ENABLE_MEMORY_REDUCTION=true
  #     - COMPUTE_SHAPES=false
  #     - MAX_PARALLEL_REQUESTS=1
  #     - NODE_OPTIONS=--max-old-space-size=4096 --max-http-header-size=32768
  #     - ENABLE_AGGRESSIVE_GC=true
  #     - DISABLE_TELEMETRY=true
  #     - HEALTH_CHECK_ENABLED=true
  #     - HEALTH_CHECK_PATH=/health
  #   networks:
  #     - ollama-network
  #   depends_on:
  #     - ollama

networks:
  ollama-network:
    name: ollama-network
    driver: bridge